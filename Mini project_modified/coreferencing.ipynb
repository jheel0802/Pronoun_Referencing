{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the directory containing the Stanford CoreNLP library\n",
    "path_to_corenlp = 'C:/Users/Admin/Desktop/sem 6/mp/stanford-corenlp-4.5.4'\n",
    "\n",
    "# Start the Stanford CoreNLP server\n",
    "nlp = StanfordCoreNLP(path_to_corenlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text to analyze\n",
    "text = \"Hetal and Priya were going to Kashmir but they forgot to bring warm clothing with them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate the text with coreference resolution\n",
    "annotated_text = nlp.annotate(text, properties={\n",
    "    'annotators': 'coref',\n",
    "    'outputFormat': 'json',\n",
    "    'timeout': 15000,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreferences = []\n",
    "json_load = (json.loads(annotated_text))\n",
    "# print(json_load['corefs'].values())\n",
    "for cluster in json_load['corefs'].values():\n",
    "    word_group = []\n",
    "    for connections in cluster:\n",
    "        word_group.append(connections['text'])\n",
    "    coreferences.append(word_group)\n",
    "print(\"text:\",text)\n",
    "print(coreferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreferences = []\n",
    "# for coref in annotated_text.corefs:\n",
    "#     for mention in coref['mentions']:\n",
    "#         if mention['isRepresentativeMention']:\n",
    "#             representative_text = mention['text']\n",
    "#             representative_cluster_id = coref['corefClusterID']\n",
    "#         else:\n",
    "#             coreferences.append({'text': mention['text'], 'cluster_id': coref['corefClusterID'], 'arepresentative_text': representative_text, 'representative_cluster_id': representative_cluster_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print the pronoun references\n",
    "# for coref in coreferences:\n",
    "#     cluster_id = coref['cluster_id']\n",
    "#     for sentence in annotated_text['sentences']:\n",
    "#         for word in sentence['tokens']:\n",
    "#             if 'corefClusterID' in word and word['corefClusterID'] == cluster_id:\n",
    "#                 print(f\"{coref['text']} refers to {word['originalText']} in sentence '{sentence['text']}'\")\n",
    "#                 break\n",
    "\n",
    "# close the CoreNLP client\n",
    "# nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sentence\n",
    "sentence = \"Ram and Rahul encountered beautiful forest, breathtaking waterfall and mountainside view where they discovered gold and silver.\"\n",
    "x = []\n",
    "# process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# start the reverse traversal from the end of the sentence\n",
    "i = len(doc.sentences[0].words) - 1\n",
    "print(i)\n",
    "while i >= 0:\n",
    "    # if a word has deprel \"conj\", start a second pointer j to find the subject deprel\n",
    "    if doc.sentences[0].words[i].deprel == \"conj\":\n",
    "        # print(\"hit:\",i)\n",
    "        j = i - 1\n",
    "        while j >= 0:\n",
    "            # if a subject deprel is found, print the words from j to i\n",
    "            if doc.sentences[0].words[j].deprel.startswith(\"obl\") or doc.sentences[0].words[j].deprel.startswith(\"nsubj\") or doc.sentences[0].words[j].deprel.startswith(\"obj\"):\n",
    "                words = [word.text for word in doc.sentences[0].words[j:i+1]]\n",
    "                x.append(\" \".join(words))\n",
    "                break\n",
    "            j -= 1\n",
    "        # start the next traversal from the position before the subject deprel word\n",
    "        i = j - 1\n",
    "    else:\n",
    "        # print(\"else hit:\",i)\n",
    "        i -= 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# download the English language model\n",
    "stanza.download('en')\n",
    "# initialize the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sentence\n",
    "sentence = \"Ram and Rahul were told to come back before sunset but they chose to ignore the warnings.\"\n",
    "x = []\n",
    "# process the sentence\n",
    "doc = nlp_(sentence)\n",
    "\n",
    "# start the reverse traversal from the end of the sentence\n",
    "i = len(doc.sentences[0].words) - 1\n",
    "print(i)\n",
    "while i >= 0:\n",
    "    # if a word has deprel \"conj\", start a second pointer j to find the subject deprel\n",
    "    if doc.sentences[0].words[i].deprel == \"conj\":\n",
    "        # print(\"hit:\",i)\n",
    "        j = i - 1\n",
    "        while j >= 0:\n",
    "            # if a subject deprel is found, print the words from j to i\n",
    "            if doc.sentences[0].words[j].deprel.startswith(\"obl\") or doc.sentences[0].words[j].deprel.startswith(\"nsubj\") or doc.sentences[0].words[j].deprel.startswith(\"obj\"):\n",
    "                words = [word.text for word in doc.sentences[0].words[j:i+1]]\n",
    "                x.append(\" \".join(words))\n",
    "                break\n",
    "            j -= 1\n",
    "        # start the next traversal from the position before the subject deprel word\n",
    "        i = j - 1\n",
    "    else:\n",
    "        # print(\"else hit:\",i)\n",
    "        i -= 1\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
